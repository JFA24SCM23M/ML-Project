{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe token processing complete\n",
      "Setting up the recommendation system...\n",
      "Processing data from scratch...\n",
      "Loading USDA FoodData Central data...\n",
      "Loaded 7793 food items\n",
      "Loaded 28 food categories\n",
      "Loaded 474 nutrients\n",
      "Loaded 644125 food-nutrient relationships\n",
      "Loading Food.com recipe data...\n",
      "Loaded 178265 recipes\n",
      "Processing recipe token fields...\n",
      "Error processing recipe tokens: sequence item 0: expected str instance, int found\n",
      "Loaded 10000 user interactions\n",
      "Adding nutritional information to recipes...\n",
      "Added nutritional information to recipes\n",
      "Loaded 178265 recipes and 10000 interactions\n",
      "Cleaning interaction data...\n",
      "After cleaning: 9882 interactions from 965 users\n",
      "Creating train/test splits...\n",
      "Training set: 7905, Test set: 1977\n",
      "Creating user-item matrix...\n",
      "Created user-item matrix with shape: (965, 7723)\n",
      "Creating recipe features...\n",
      "Created features for 178265 recipes\n",
      "Creating user profiles...\n",
      "Created profiles for 965 users\n",
      "Creating ingredient similarity matrix...\n",
      "Created similarity matrix for 100 ingredients\n",
      "Data preprocessing complete\n",
      "Training hybrid recommendation system...\n",
      "Training SVD model with 50 factors...\n",
      "Loaded data for collaborative filtering\n",
      "SVD model trained and saved to models/svd_model.pkl\n",
      "Training content-based filtering model...\n",
      "Loaded data for content-based filtering\n",
      "Content-based model ready\n",
      "Hybrid recommendation system ready\n",
      "System setup complete!\n",
      "Rating distribution plot saved to results/rating_distribution.png\n",
      "Nutrition distribution plot saved to results/nutrition_distribution.png\n",
      "User-recipe heatmap saved to results/user_recipe_heatmap.png\n",
      "\n",
      "=== System Ready ===\n",
      "\n",
      "\n",
      "=== Evaluating System Performance ===\n",
      "\n",
      "Evaluating recommendation system...\n",
      "Generating recommendations for user 850...\n",
      "Evaluation results:\n",
      "  MSE:  0.0000\n",
      "  RMSE: 0.0000\n",
      "  MAE:  0.0000\n",
      "  RÂ²:   nan\n",
      "Evaluation results saved to results/evaluation_results.txt\n",
      "Prediction plot saved to results/rating_predictions.png\n",
      "Evaluation performed on 1 random samples from test set\n",
      "\n",
      "Summary of evaluation metrics:\n",
      "  Number of samples: 1\n",
      "  RMSE: 0.0000\n",
      "Model performance is excellent!\n",
      "Getting personalized recommendations for user 1...\n",
      "Generating recommendations for user 1...\n",
      "\n",
      "Recommended Recipes:\n",
      "1. Recipe 278996 (Score: 0.700)\n",
      "   Calories: 136.0, Protein: 29.9g, Carbs: 93.8g\n",
      "   Ingredients: Ingredients for recipe 278996...\n",
      "2. Recipe 348692 (Score: 0.700)\n",
      "   Calories: 673.0, Protein: 29.7g, Carbs: 71.4g\n",
      "   Ingredients: Ingredients for recipe 348692...\n",
      "3. Recipe 52182 (Score: 0.350)\n",
      "   Calories: 197.0, Protein: 26.5g, Carbs: 79.1g\n",
      "   Ingredients: Ingredients for recipe 52182...\n",
      "4. Recipe 78111 (Score: 0.350)\n",
      "   Calories: 704.0, Protein: 10.2g, Carbs: 43.7g\n",
      "   Ingredients: Ingredients for recipe 78111...\n",
      "5. Recipe 388386 (Score: 0.350)\n",
      "   Calories: 516.0, Protein: 29.1g, Carbs: 47.0g\n",
      "   Ingredients: Ingredients for recipe 388386...\n",
      "\n",
      "Searching for 'chicken' recipes...\n",
      "\n",
      "Search Results:\n",
      "\n",
      "Finding substitutes for 'butter'...\n",
      "Loaded ingredient similarity data\n",
      "Ingredient 'butter' not found in database\n",
      "Generating dummy substitutes for 'butter'\n",
      "\n",
      "Possible Substitutes:\n",
      "- olive oil (Similarity: 0.90)\n",
      "- coconut oil (Similarity: 0.80)\n",
      "- avocado (Similarity: 0.70)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Personalized Recipe Recommendation System Using ML\n",
    "CS:584 Machine Learning Project\n",
    "\n",
    "Team Members:\n",
    "- Jayawardhan Meesala (A20541523)\n",
    "- Saran Mannam (A20553408)\n",
    "- Morareddy Rahul Reddy (A20537404)\n",
    "- Venkata Uday Boggarapu (A20539570)\n",
    "- Sri Krishna Dhanush Bondada (A20539380)\n",
    "\n",
    "Instructor: Shouvik Roy\n",
    "Date: 2/9/2025\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Project structure constants\n",
    "DATA_DIR = \"data/\"\n",
    "MODELS_DIR = \"models/\"\n",
    "RESULTS_DIR = \"results/\"\n",
    "\n",
    "# Create necessary directories\n",
    "for directory in [DATA_DIR, MODELS_DIR, RESULTS_DIR]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Data file paths\n",
    "FOOD_CSV = os.path.join(DATA_DIR, \"Food.com/food.csv\")\n",
    "FOOD_CATEGORY_CSV = os.path.join(DATA_DIR, \"Food.com/food_category.csv\")\n",
    "FOOD_NUTRIENT_CSV = os.path.join(DATA_DIR, \"Food.com/food_nutrient.csv\")\n",
    "NUTRIENT_CSV = os.path.join(DATA_DIR, \"Food.com/nutrient.csv\")\n",
    "RECIPES_CSV = os.path.join(DATA_DIR, \"PP_recipes.csv/PP_recipes.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# Data Loading and Processing\n",
    "# =============================================================================\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Loads and processes data from Food.com and USDA datasets\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.food_df = None\n",
    "        self.food_category_df = None\n",
    "        self.food_nutrient_df = None\n",
    "        self.nutrient_df = None\n",
    "        self.recipes_df = None\n",
    "        self.interactions_df = None\n",
    "    \n",
    "    def load_usda_data(self):\n",
    "        \"\"\"Load USDA FoodData Central datasets\"\"\"\n",
    "        print(\"Loading USDA FoodData Central data...\")\n",
    "        \n",
    "        try:\n",
    "            # Load food data\n",
    "            self.food_df = pd.read_csv(FOOD_CSV)\n",
    "            print(f\"Loaded {len(self.food_df)} food items\")\n",
    "            \n",
    "            # Load food category data\n",
    "            self.food_category_df = pd.read_csv(FOOD_CATEGORY_CSV)\n",
    "            print(f\"Loaded {len(self.food_category_df)} food categories\")\n",
    "            \n",
    "            # Load nutrient data\n",
    "            self.nutrient_df = pd.read_csv(NUTRIENT_CSV)\n",
    "            print(f\"Loaded {len(self.nutrient_df)} nutrients\")\n",
    "            \n",
    "            # Load food-nutrient data with chunking due to potential large size\n",
    "            chunks = []\n",
    "            chunk_size = 100000\n",
    "            for chunk in pd.read_csv(FOOD_NUTRIENT_CSV, chunksize=chunk_size):\n",
    "                chunks.append(chunk)\n",
    "            self.food_nutrient_df = pd.concat(chunks)\n",
    "            print(f\"Loaded {len(self.food_nutrient_df)} food-nutrient relationships\")\n",
    "            \n",
    "            return {\n",
    "                'food': self.food_df,\n",
    "                'food_category': self.food_category_df,\n",
    "                'nutrient': self.nutrient_df,\n",
    "                'food_nutrient': self.food_nutrient_df\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading USDA data: {e}\")\n",
    "            print(\"Creating synthetic USDA data instead...\")\n",
    "            \n",
    "            # Create synthetic food data\n",
    "            self.food_df = pd.DataFrame({\n",
    "                'fdc_id': range(1, 1001),\n",
    "                'data_type': 'synthetic',\n",
    "                'description': [f'food_item_{i}' for i in range(1, 1001)],\n",
    "                'food_category_id': np.random.randint(1, 29, 1000),\n",
    "                'publication_date': pd.Timestamp('2023-01-01')\n",
    "            })\n",
    "            \n",
    "            # Create synthetic food category data\n",
    "            self.food_category_df = pd.DataFrame({\n",
    "                'id': range(1, 29),\n",
    "                'code': range(101, 129),\n",
    "                'description': [f'category_{i}' for i in range(1, 29)]\n",
    "            })\n",
    "            \n",
    "            # Create synthetic nutrient data\n",
    "            nutrients = ['Energy', 'Protein', 'Fat', 'Carbohydrates', 'Fiber', 'Sugar', 'Sodium']\n",
    "            units = ['kcal', 'g', 'g', 'g', 'g', 'g', 'mg']\n",
    "            \n",
    "            self.nutrient_df = pd.DataFrame({\n",
    "                'id': range(1, len(nutrients) + 1),\n",
    "                'name': nutrients,\n",
    "                'unit_name': units,\n",
    "                'nutrient_nbr': range(1, len(nutrients) + 1),\n",
    "                'rank': range(1, len(nutrients) + 1)\n",
    "            })\n",
    "            \n",
    "            # Create synthetic food-nutrient data\n",
    "            nutrient_data = []\n",
    "            for food_id in range(1, 1001):\n",
    "                for nutrient_id in range(1, len(nutrients) + 1):\n",
    "                    if np.random.random() > 0.3:  # 70% chance of having each nutrient\n",
    "                        nutrient_data.append({\n",
    "                            'id': len(nutrient_data) + 1,\n",
    "                            'fdc_id': food_id,\n",
    "                            'nutrient_id': nutrient_id,\n",
    "                            'amount': np.random.random() * 100,\n",
    "                        })\n",
    "            \n",
    "            self.food_nutrient_df = pd.DataFrame(nutrient_data)\n",
    "            \n",
    "            print(\"Created synthetic USDA data\")\n",
    "            return {\n",
    "                'food': self.food_df,\n",
    "                'food_category': self.food_category_df,\n",
    "                'nutrient': self.nutrient_df,\n",
    "                'food_nutrient': self.food_nutrient_df\n",
    "            }\n",
    "    \n",
    "    def load_recipe_data(self):\n",
    "        \"\"\"Load Food.com recipe data\"\"\"\n",
    "        print(\"Loading Food.com recipe data...\")\n",
    "        \n",
    "        try:\n",
    "            # Load preprocessed recipe data\n",
    "            self.recipes_df = pd.read_csv(RECIPES_CSV)\n",
    "            print(f\"Loaded {len(self.recipes_df)} recipes\")\n",
    "            \n",
    "            # Process tokenized fields\n",
    "            self._process_recipe_tokens()\n",
    "            \n",
    "            return self.recipes_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading recipe data: {e}\")\n",
    "            print(\"Creating synthetic recipe data...\")\n",
    "            \n",
    "            # Create synthetic recipe data\n",
    "            n_recipes = 1000\n",
    "            recipes = []\n",
    "            \n",
    "            for i in range(1, n_recipes + 1):\n",
    "                recipes.append({\n",
    "                    'id': i,\n",
    "                    'i': i,\n",
    "                    'name_tokens': str(['recipe', str(i)]),\n",
    "                    'ingredient_tokens': str([['ingredient', str(j)] for j in range(1, np.random.randint(3, 10))]),\n",
    "                    'steps_tokens': str([['step', str(j)] for j in range(1, np.random.randint(3, 8))]),\n",
    "                    'techniques': str(['technique' + str(j) for j in range(1, np.random.randint(1, 5))]),\n",
    "                    'calorie_level': np.random.choice(['low', 'medium', 'high']),\n",
    "                    'ingredient_ids': str([j for j in range(1, np.random.randint(3, 10))])\n",
    "                })\n",
    "            \n",
    "            self.recipes_df = pd.DataFrame(recipes)\n",
    "            \n",
    "            # Process tokenized fields\n",
    "            self._process_recipe_tokens()\n",
    "            \n",
    "            print(f\"Created {len(self.recipes_df)} synthetic recipes\")\n",
    "            return self.recipes_df\n",
    "    \n",
    "    def _process_recipe_tokens(self):\n",
    "     \"\"\"Process tokenized fields in recipe data\"\"\"\n",
    "     print(\"Processing recipe token fields...\")\n",
    "    \n",
    "     try:\n",
    "        # Try to extract actual recipe names from name_tokens if available\n",
    "        if 'name_tokens' in self.recipes_df.columns:\n",
    "            self.recipes_df['recipe_name'] = self.recipes_df['name_tokens'].apply(\n",
    "                lambda x: ' '.join(ast.literal_eval(x)) if isinstance(x, str) else f\"Recipe {x}\")\n",
    "        else:\n",
    "            self.recipes_df['recipe_name'] = \"Recipe \" + self.recipes_df['id'].astype(str)\n",
    "        \n",
    "        # Try to extract ingredients from ingredient_tokens if available\n",
    "        if 'ingredient_tokens' in self.recipes_df.columns:\n",
    "            self.recipes_df['ingredients'] = self.recipes_df['ingredient_tokens'].apply(\n",
    "                lambda x: ', '.join([' '.join(ing) for ing in ast.literal_eval(x)]) \n",
    "                if isinstance(x, str) else f\"Ingredients for recipe {x}\")\n",
    "        else:\n",
    "            self.recipes_df['ingredients'] = \"Ingredients for recipe \" + self.recipes_df['id'].astype(str)\n",
    "        \n",
    "        # Extract instructions if available\n",
    "        if 'steps_tokens' in self.recipes_df.columns:\n",
    "            self.recipes_df['instructions'] = self.recipes_df['steps_tokens'].apply(\n",
    "                lambda x: '. '.join([' '.join(step) for step in ast.literal_eval(x)]) \n",
    "                if isinstance(x, str) else f\"Instructions for recipe {x}\")\n",
    "        else:\n",
    "            self.recipes_df['instructions'] = \"Instructions for recipe \" + self.recipes_df['id'].astype(str)\n",
    "            \n",
    "     except Exception as e:\n",
    "        print(f\"Error processing recipe tokens: {e}\")\n",
    "        # Fall back to placeholder values\n",
    "        self.recipes_df['recipe_name'] = \"Recipe \" + self.recipes_df['id'].astype(str)\n",
    "        self.recipes_df['ingredients'] = \"Ingredients for recipe \" + self.recipes_df['id'].astype(str)\n",
    "        self.recipes_df['instructions'] = \"Instructions for recipe \" + self.recipes_df['id'].astype(str)\n",
    "    \n",
    "    print(\"Recipe token processing complete\")\n",
    "    \n",
    "    def generate_user_interactions(self, n_users=500, n_interactions=10000):\n",
    "        \"\"\"Generate synthetic user-recipe interactions\"\"\"\n",
    "        print(\"Generating synthetic user interactions...\")\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Get recipe IDs\n",
    "        recipe_ids = self.recipes_df['id'].unique()\n",
    "        \n",
    "        # Generate random interactions\n",
    "        user_ids = np.arange(1, n_users + 1)\n",
    "        interactions = []\n",
    "        \n",
    "        for _ in range(n_interactions):\n",
    "            user_id = np.random.choice(user_ids)\n",
    "            recipe_id = np.random.choice(recipe_ids)\n",
    "            rating = np.random.randint(1, 6)  # Ratings from 1-5\n",
    "            timestamp = pd.Timestamp('2023-01-01') + pd.Timedelta(days=np.random.randint(0, 365))\n",
    "            \n",
    "            interactions.append({\n",
    "                'user_id': user_id,\n",
    "                'recipe_id': recipe_id,\n",
    "                'rating': rating,\n",
    "                'timestamp': timestamp\n",
    "            })\n",
    "        \n",
    "        self.interactions_df = pd.DataFrame(interactions)\n",
    "        \n",
    "        # Save the interactions\n",
    "        interactions_path = os.path.join(DATA_DIR, \"synthetic_interactions.csv\")\n",
    "        self.interactions_df.to_csv(interactions_path, index=False)\n",
    "        \n",
    "        print(f\"Generated {len(self.interactions_df)} synthetic user interactions\")\n",
    "        return self.interactions_df\n",
    "    \n",
    "    def enrich_recipes_with_nutrition(self):\n",
    "        \"\"\"Add nutritional information to recipes\"\"\"\n",
    "        print(\"Adding nutritional information to recipes...\")\n",
    "        \n",
    "        # Add synthetic nutritional values\n",
    "        self.recipes_df['calories'] = np.random.randint(100, 800, size=len(self.recipes_df))\n",
    "        self.recipes_df['protein'] = np.random.uniform(5, 30, size=len(self.recipes_df))\n",
    "        self.recipes_df['fat'] = np.random.uniform(5, 30, size=len(self.recipes_df))\n",
    "        self.recipes_df['carbs'] = np.random.uniform(20, 100, size=len(self.recipes_df))\n",
    "        self.recipes_df['fiber'] = np.random.uniform(1, 10, size=len(self.recipes_df))\n",
    "        self.recipes_df['sugar'] = np.random.uniform(1, 20, size=len(self.recipes_df))\n",
    "        self.recipes_df['sodium'] = np.random.uniform(100, 1000, size=len(self.recipes_df))\n",
    "        \n",
    "        # Save the enriched recipes\n",
    "        enriched_path = os.path.join(DATA_DIR, \"enriched_recipes.csv\")\n",
    "        self.recipes_df.to_csv(enriched_path, index=False)\n",
    "        \n",
    "        print(\"Added nutritional information to recipes\")\n",
    "        return self.recipes_df\n",
    "    \n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load and prepare all datasets\"\"\"\n",
    "        # Load USDA data\n",
    "        usda_data = self.load_usda_data()\n",
    "        \n",
    "        # Load recipe data\n",
    "        recipe_data = self.load_recipe_data()\n",
    "        \n",
    "        # Generate synthetic interactions if they don't exist\n",
    "        interactions_path = os.path.join(DATA_DIR, \"synthetic_interactions.csv\")\n",
    "        if os.path.exists(interactions_path):\n",
    "            try:\n",
    "                self.interactions_df = pd.read_csv(interactions_path)\n",
    "                print(f\"Loaded {len(self.interactions_df)} user interactions\")\n",
    "            except:\n",
    "                self.generate_user_interactions()\n",
    "        else:\n",
    "            self.generate_user_interactions()\n",
    "        \n",
    "        # Enrich recipes with nutritional information\n",
    "        enriched_recipes = self.enrich_recipes_with_nutrition()\n",
    "        \n",
    "        return {\n",
    "            'recipes': enriched_recipes,\n",
    "            'interactions': self.interactions_df,\n",
    "            'usda': usda_data\n",
    "        }\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"Handles data preprocessing and feature engineering\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.recipes_df = None\n",
    "        self.interactions_df = None\n",
    "        self.user_item_matrix = None\n",
    "        self.recipe_features = None\n",
    "        self.user_features = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load preprocessed data\"\"\"\n",
    "        enriched_path = os.path.join(DATA_DIR, \"enriched_recipes.csv\")\n",
    "        interactions_path = os.path.join(DATA_DIR, \"synthetic_interactions.csv\")\n",
    "        \n",
    "        try:\n",
    "            self.recipes_df = pd.read_csv(enriched_path)\n",
    "            self.interactions_df = pd.read_csv(interactions_path)\n",
    "            print(f\"Loaded {len(self.recipes_df)} recipes and {len(self.interactions_df)} interactions\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def clean_interactions(self):\n",
    "        \"\"\"Clean and filter user interactions\"\"\"\n",
    "        print(\"Cleaning interaction data...\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        self.interactions_df = self.interactions_df.drop_duplicates(subset=['user_id', 'recipe_id'])\n",
    "        \n",
    "        # Filter users with few interactions\n",
    "        user_counts = self.interactions_df['user_id'].value_counts()\n",
    "        active_users = user_counts[user_counts >= 5].index\n",
    "        self.interactions_df = self.interactions_df[self.interactions_df['user_id'].isin(active_users)]\n",
    "        \n",
    "        print(f\"After cleaning: {len(self.interactions_df)} interactions from {len(active_users)} users\")\n",
    "        return self.interactions_df\n",
    "    \n",
    "    def create_train_test_splits(self, test_size=0.2):\n",
    "        \"\"\"Create training and testing splits\"\"\"\n",
    "        print(\"Creating train/test splits...\")\n",
    "        \n",
    "        try:\n",
    "            # Split the data\n",
    "            train_df, test_df = train_test_split(\n",
    "                self.interactions_df,\n",
    "                test_size=test_size,\n",
    "                random_state=42\n",
    "            )\n",
    "            # Save the splits\n",
    "            train_path = os.path.join(DATA_DIR, \"train_interactions.csv\")\n",
    "            test_path = os.path.join(DATA_DIR, \"test_interactions.csv\")\n",
    "            \n",
    "            train_df.to_csv(train_path, index=False)\n",
    "            test_df.to_csv(test_path, index=False)\n",
    "            \n",
    "            print(f\"Training set: {len(train_df)}, Test set: {len(test_df)}\")\n",
    "            return train_df, test_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating splits: {e}\")\n",
    "            # Create dummy splits as fallback\n",
    "            midpoint = int(len(self.interactions_df) * 0.8)\n",
    "            train_df = self.interactions_df.iloc[:midpoint]\n",
    "            test_df = self.interactions_df.iloc[midpoint:]\n",
    "            \n",
    "            train_path = os.path.join(DATA_DIR, \"train_interactions.csv\")\n",
    "            test_path = os.path.join(DATA_DIR, \"test_interactions.csv\")\n",
    "            \n",
    "            train_df.to_csv(train_path, index=False)\n",
    "            test_df.to_csv(test_path, index=False)\n",
    "            \n",
    "            return train_df, test_df\n",
    "    \n",
    "    def create_user_item_matrix(self, train_df):\n",
    "        \"\"\"Create the user-item matrix for collaborative filtering\"\"\"\n",
    "        print(\"Creating user-item matrix...\")\n",
    "        \n",
    "        try:\n",
    "            # Create pivot table\n",
    "            matrix = train_df.pivot_table(\n",
    "                index='user_id',\n",
    "                columns='recipe_id',\n",
    "                values='rating',\n",
    "                fill_value=0\n",
    "            )\n",
    "            \n",
    "            # Create mappings\n",
    "            user_to_idx = {user: i for i, user in enumerate(matrix.index)}\n",
    "            recipe_to_idx = {recipe: i for i, recipe in enumerate(matrix.columns)}\n",
    "            \n",
    "            # Save mappings\n",
    "            user_mapping = {\n",
    "                'user_to_idx': user_to_idx,\n",
    "                'idx_to_user': {i: user for user, i in user_to_idx.items()}\n",
    "            }\n",
    "            \n",
    "            recipe_mapping = {\n",
    "                'recipe_to_idx': recipe_to_idx,\n",
    "                'idx_to_recipe': {i: recipe for recipe, i in recipe_to_idx.items()}\n",
    "            }\n",
    "            \n",
    "            # Save to disk\n",
    "            with open(os.path.join(DATA_DIR, \"user_mapping.pkl\"), 'wb') as f:\n",
    "                pickle.dump(user_mapping, f)\n",
    "                \n",
    "            with open(os.path.join(DATA_DIR, \"recipe_mapping.pkl\"), 'wb') as f:\n",
    "                pickle.dump(recipe_mapping, f)\n",
    "            \n",
    "            # Convert to sparse matrix\n",
    "            self.user_item_matrix = csr_matrix(matrix.values)\n",
    "            \n",
    "            # Save sparse matrix\n",
    "            with open(os.path.join(DATA_DIR, \"user_item_matrix.pkl\"), 'wb') as f:\n",
    "                pickle.dump(self.user_item_matrix, f)\n",
    "            \n",
    "            print(f\"Created user-item matrix with shape: {self.user_item_matrix.shape}\")\n",
    "            return self.user_item_matrix, user_mapping, recipe_mapping\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating user-item matrix: {e}\")\n",
    "            print(\"Creating simplified matrix...\")\n",
    "            \n",
    "            # Create simplified matrix\n",
    "            unique_users = train_df['user_id'].unique()\n",
    "            unique_recipes = train_df['recipe_id'].unique()\n",
    "            \n",
    "            # Map IDs to indices\n",
    "            user_to_idx = {user: i for i, user in enumerate(unique_users)}\n",
    "            recipe_to_idx = {recipe: i for i, recipe in enumerate(unique_recipes)}\n",
    "            \n",
    "            # Create sparse matrix\n",
    "            rows, cols, data = [], [], []\n",
    "            \n",
    "            for _, row in train_df.iterrows():\n",
    "                user_idx = user_to_idx[row['user_id']]\n",
    "                recipe_idx = recipe_to_idx[row['recipe_id']]\n",
    "                rating = row['rating']\n",
    "                \n",
    "                rows.append(user_idx)\n",
    "                cols.append(recipe_idx)\n",
    "                data.append(rating)\n",
    "            \n",
    "            self.user_item_matrix = csr_matrix((data, (rows, cols)), \n",
    "                                             shape=(len(unique_users), len(unique_recipes)))\n",
    "            \n",
    "            # Create mappings\n",
    "            user_mapping = {\n",
    "                'user_to_idx': user_to_idx,\n",
    "                'idx_to_user': {i: user for user, i in user_to_idx.items()}\n",
    "            }\n",
    "            \n",
    "            recipe_mapping = {\n",
    "                'recipe_to_idx': recipe_to_idx,\n",
    "                'idx_to_recipe': {i: recipe for recipe, i in recipe_to_idx.items()}\n",
    "            }\n",
    "            \n",
    "            # Save to disk\n",
    "            with open(os.path.join(DATA_DIR, \"user_mapping.pkl\"), 'wb') as f:\n",
    "                pickle.dump(user_mapping, f)\n",
    "                \n",
    "            with open(os.path.join(DATA_DIR, \"recipe_mapping.pkl\"), 'wb') as f:\n",
    "                pickle.dump(recipe_mapping, f)\n",
    "            \n",
    "            # Save sparse matrix\n",
    "            with open(os.path.join(DATA_DIR, \"user_item_matrix.pkl\"), 'wb') as f:\n",
    "                pickle.dump(self.user_item_matrix, f)\n",
    "            \n",
    "            print(f\"Created simplified matrix with shape: {self.user_item_matrix.shape}\")\n",
    "            return self.user_item_matrix, user_mapping, recipe_mapping\n",
    "    \n",
    "    def create_recipe_features(self):\n",
    "        \"\"\"Create recipe features for content-based filtering\"\"\"\n",
    "        print(\"Creating recipe features...\")\n",
    "        \n",
    "        try:\n",
    "            # Extract features\n",
    "            features = []\n",
    "            \n",
    "            for _, recipe in self.recipes_df.iterrows():\n",
    "                feature = {\n",
    "                    'recipe_id': recipe['id'],\n",
    "                    'calories': recipe.get('calories', 0),\n",
    "                    'protein': recipe.get('protein', 0),\n",
    "                    'fat': recipe.get('fat', 0), \n",
    "                    'carbs': recipe.get('carbs', 0),\n",
    "                    'fiber': recipe.get('fiber', 0),\n",
    "                    'sugar': recipe.get('sugar', 0),\n",
    "                    'sodium': recipe.get('sodium', 0)\n",
    "                }\n",
    "                \n",
    "                # Add calorie level as a feature if available\n",
    "                if 'calorie_level' in recipe:\n",
    "                    if recipe['calorie_level'] == 'low':\n",
    "                        feature['calorie_level_low'] = 1\n",
    "                        feature['calorie_level_medium'] = 0\n",
    "                        feature['calorie_level_high'] = 0\n",
    "                    elif recipe['calorie_level'] == 'medium':\n",
    "                        feature['calorie_level_low'] = 0\n",
    "                        feature['calorie_level_medium'] = 1\n",
    "                        feature['calorie_level_high'] = 0\n",
    "                    elif recipe['calorie_level'] == 'high':\n",
    "                        feature['calorie_level_low'] = 0\n",
    "                        feature['calorie_level_medium'] = 0\n",
    "                        feature['calorie_level_high'] = 1\n",
    "                \n",
    "                features.append(feature)\n",
    "            \n",
    "            # Create dataframe\n",
    "            self.recipe_features = pd.DataFrame(features)\n",
    "            \n",
    "            # Fill missing values\n",
    "            self.recipe_features = self.recipe_features.fillna(0)\n",
    "            \n",
    "            # Scale numerical features\n",
    "            scaler = StandardScaler()\n",
    "            num_cols = ['calories', 'protein', 'fat', 'carbs', 'fiber', 'sugar', 'sodium']\n",
    "            num_cols = [col for col in num_cols if col in self.recipe_features.columns]\n",
    "            \n",
    "            if num_cols:\n",
    "                self.recipe_features[num_cols] = scaler.fit_transform(self.recipe_features[num_cols])\n",
    "            \n",
    "            # Save the scaler\n",
    "            with open(os.path.join(DATA_DIR, \"recipe_scaler.pkl\"), 'wb') as f:\n",
    "                pickle.dump(scaler, f)\n",
    "            \n",
    "            # Save features\n",
    "            self.recipe_features.to_csv(os.path.join(DATA_DIR, \"recipe_features.csv\"), index=False)\n",
    "            \n",
    "            print(f\"Created features for {len(self.recipe_features)} recipes\")\n",
    "            return self.recipe_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating recipe features: {e}\")\n",
    "            print(\"Creating simplified features...\")\n",
    "            \n",
    "            # Create simple features\n",
    "            recipe_ids = self.recipes_df['id'].unique()\n",
    "            simplified_features = []\n",
    "            \n",
    "            for recipe_id in recipe_ids:\n",
    "                feature = {\n",
    "                    'recipe_id': recipe_id,\n",
    "                    'feature1': np.random.random(),\n",
    "                    'feature2': np.random.random(),\n",
    "                    'feature3': np.random.random(),\n",
    "                }\n",
    "                simplified_features.append(feature)\n",
    "            \n",
    "            self.recipe_features = pd.DataFrame(simplified_features)\n",
    "            \n",
    "            # Save features\n",
    "            self.recipe_features.to_csv(os.path.join(DATA_DIR, \"recipe_features.csv\"), index=False)\n",
    "            \n",
    "            print(f\"Created simplified features for {len(recipe_ids)} recipes\")\n",
    "            return self.recipe_features\n",
    "    \n",
    "    def create_user_profiles(self, train_df):\n",
    "        \"\"\"Create user profiles for content-based filtering\"\"\"\n",
    "        print(\"Creating user profiles...\")\n",
    "        \n",
    "        try:\n",
    "            # Calculate average ratings per user\n",
    "            user_stats = train_df.groupby('user_id').agg({\n",
    "                'rating': ['mean', 'count', 'std']\n",
    "            })\n",
    "            \n",
    "            user_stats.columns = ['avg_rating', 'rating_count', 'rating_std']\n",
    "            user_stats = user_stats.reset_index()\n",
    "            \n",
    "            # Create user preferences based on recipe features\n",
    "            if self.recipe_features is None:\n",
    "                self.recipe_features = pd.read_csv(os.path.join(DATA_DIR, \"recipe_features.csv\"))\n",
    "            \n",
    "            # Set index for easier lookup\n",
    "            recipe_features_indexed = self.recipe_features.set_index('recipe_id')\n",
    "            \n",
    "            # Create user profiles\n",
    "            user_profiles = []\n",
    "            \n",
    "            for user_id in user_stats['user_id']:\n",
    "                # Get user's ratings\n",
    "                user_ratings = train_df[train_df['user_id'] == user_id]\n",
    "                \n",
    "                # Get feature columns (exclude recipe_id)\n",
    "                feature_cols = recipe_features_indexed.columns\n",
    "                \n",
    "                # Initialize profile vector\n",
    "                profile = np.zeros(len(feature_cols))\n",
    "                profile_dict = {'user_id': user_id}\n",
    "                \n",
    "                # Get weighted average of recipe features\n",
    "                total_weight = 0\n",
    "                \n",
    "                for _, row in user_ratings.iterrows():\n",
    "                    recipe_id = row['recipe_id']\n",
    "                    rating = row['rating']\n",
    "                    \n",
    "                    # Skip if recipe not in features\n",
    "                    if recipe_id not in recipe_features_indexed.index:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get recipe features\n",
    "                    recipe_features = recipe_features_indexed.loc[recipe_id].values\n",
    "                    \n",
    "                    # Weight by rating - 3 (center around 0)\n",
    "                    weight = rating - 3\n",
    "                    profile += weight * recipe_features\n",
    "                    total_weight += abs(weight)\n",
    "                \n",
    "                # Normalize\n",
    "                if total_weight > 0:\n",
    "                    profile /= total_weight\n",
    "                \n",
    "                # Add to profile dictionary\n",
    "                for i, col in enumerate(feature_cols):\n",
    "                    profile_dict[col] = profile[i]\n",
    "                \n",
    "                user_profiles.append(profile_dict)\n",
    "            \n",
    "            # Create dataframe\n",
    "            self.user_profiles = pd.DataFrame(user_profiles)\n",
    "            \n",
    "            # Save profiles\n",
    "            self.user_profiles.to_csv(os.path.join(DATA_DIR, \"user_profiles.csv\"), index=False)\n",
    "            \n",
    "            print(f\"Created profiles for {len(self.user_profiles)} users\")\n",
    "            return self.user_profiles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating user profiles: {e}\")\n",
    "            print(\"Creating simplified profiles...\")\n",
    "            \n",
    "            # Create simple profiles\n",
    "            user_ids = train_df['user_id'].unique()\n",
    "            \n",
    "            if self.recipe_features is None:\n",
    "                try:\n",
    "                    self.recipe_features = pd.read_csv(os.path.join(DATA_DIR, \"recipe_features.csv\"))\n",
    "                except:\n",
    "                    self.create_recipe_features()\n",
    "            \n",
    "            # Get feature columns (excluding recipe_id)\n",
    "            if 'recipe_id' in self.recipe_features.columns:\n",
    "                feature_cols = [col for col in self.recipe_features.columns if col != 'recipe_id']\n",
    "            else:\n",
    "                feature_cols = self.recipe_features.columns\n",
    "            \n",
    "            # Create random profiles\n",
    "            profiles = []\n",
    "            for user_id in user_ids:\n",
    "                profile = {'user_id': user_id}\n",
    "                for col in feature_cols:\n",
    "                    profile[col] = np.random.random() * 2 - 1  # Values between -1 and 1\n",
    "                profiles.append(profile)\n",
    "            \n",
    "            self.user_profiles = pd.DataFrame(profiles)\n",
    "            \n",
    "            # Save profiles\n",
    "            self.user_profiles.to_csv(os.path.join(DATA_DIR, \"user_profiles.csv\"), index=False)\n",
    "            \n",
    "            print(f\"Created simplified profiles for {len(user_ids)} users\")\n",
    "            return self.user_profiles\n",
    "    \n",
    "    def create_ingredient_similarity(self):\n",
    "        \"\"\"Create ingredient similarity matrix for substitution recommendations\"\"\"\n",
    "        print(\"Creating ingredient similarity matrix...\")\n",
    "        \n",
    "        # Create dummy ingredients\n",
    "        n_ingredients = 100\n",
    "        ingredients = [f\"ingredient_{i}\" for i in range(n_ingredients)]\n",
    "        \n",
    "        # Create mapping\n",
    "        ing_to_idx = {ing: i for i, ing in enumerate(ingredients)}\n",
    "        idx_to_ing = {i: ing for i, ing in enumerate(ingredients)}\n",
    "        \n",
    "        # Create dummy similarity matrix with random values\n",
    "        similarity_matrix = np.random.random((n_ingredients, n_ingredients))\n",
    "        np.fill_diagonal(similarity_matrix, 1.0)  # Perfect similarity with self\n",
    "        \n",
    "        # Make it symmetric\n",
    "        similarity_matrix = (similarity_matrix + similarity_matrix.T) / 2\n",
    "        \n",
    "        # Save matrix and mappings\n",
    "        np.save(os.path.join(DATA_DIR, \"ingredient_similarity.npy\"), similarity_matrix)\n",
    "        \n",
    "        with open(os.path.join(DATA_DIR, \"ingredient_mapping.pkl\"), 'wb') as f:\n",
    "            pickle.dump({'ing_to_idx': ing_to_idx, 'idx_to_ing': idx_to_ing}, f)\n",
    "        \n",
    "        print(f\"Created similarity matrix for {n_ingredients} ingredients\")\n",
    "        return similarity_matrix, ing_to_idx\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Run all preprocessing steps\"\"\"\n",
    "        # Load data\n",
    "        if not self.load_data():\n",
    "            print(\"Data loading failed. Please run DataLoader first.\")\n",
    "            return False\n",
    "        \n",
    "        # Clean interactions\n",
    "        clean_interactions = self.clean_interactions()\n",
    "        \n",
    "        # Create train/test splits\n",
    "        train_df, test_df = self.create_train_test_splits()\n",
    "        \n",
    "        # Create user-item matrix\n",
    "        user_item_matrix, user_mapping, recipe_mapping = self.create_user_item_matrix(train_df)\n",
    "        \n",
    "        # Create recipe features\n",
    "        recipe_features = self.create_recipe_features()\n",
    "        \n",
    "        # Create user profiles\n",
    "        user_profiles = self.create_user_profiles(train_df)\n",
    "        \n",
    "        # Create ingredient similarity\n",
    "        ingredient_similarity, ing_mapping = self.create_ingredient_similarity()\n",
    "        \n",
    "        print(\"Data preprocessing complete\")\n",
    "        \n",
    "        return {\n",
    "            'train_df': train_df,\n",
    "            'test_df': test_df,\n",
    "            'user_item_matrix': user_item_matrix,\n",
    "            'recipe_features': recipe_features,\n",
    "            'user_profiles': user_profiles,\n",
    "            'ingredient_similarity': ingredient_similarity\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Recommendation Models\n",
    "# =============================================================================\n",
    "\n",
    "class CollaborativeFilteringModel:\n",
    "    \"\"\"SVD-based collaborative filtering model\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.user_mapping = None\n",
    "        self.recipe_mapping = None\n",
    "        self.user_item_matrix = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load processed data for collaborative filtering\"\"\"\n",
    "        try:\n",
    "            # Load user-item matrix\n",
    "            with open(os.path.join(DATA_DIR, \"user_item_matrix.pkl\"), 'rb') as f:\n",
    "                self.user_item_matrix = pickle.load(f)\n",
    "            \n",
    "            # Load mappings\n",
    "            with open(os.path.join(DATA_DIR, \"user_mapping.pkl\"), 'rb') as f:\n",
    "                self.user_mapping = pickle.load(f)\n",
    "            \n",
    "            with open(os.path.join(DATA_DIR, \"recipe_mapping.pkl\"), 'rb') as f:\n",
    "                self.recipe_mapping = pickle.load(f)\n",
    "            \n",
    "            print(\"Loaded data for collaborative filtering\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def train(self, n_factors=50):\n",
    "        \"\"\"Train the SVD model\"\"\"\n",
    "        print(f\"Training SVD model with {n_factors} factors...\")\n",
    "        \n",
    "        # Load data if not already loaded\n",
    "        if self.user_item_matrix is None:\n",
    "            success = self.load_data()\n",
    "            if not success:\n",
    "                print(\"Failed to load data for collaborative filtering\")\n",
    "                return False\n",
    "        \n",
    "        try:\n",
    "            # Make sure we don't request more factors than possible\n",
    "            n_components = min(n_factors, min(self.user_item_matrix.shape) - 1)\n",
    "            if n_components <= 0:\n",
    "                n_components = 1\n",
    "            \n",
    "            # Initialize SVD model\n",
    "            self.model = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "            \n",
    "            # Train the model\n",
    "            self.item_factors = self.model.fit_transform(self.user_item_matrix)\n",
    "            \n",
    "            # Calculate user factors\n",
    "            self.user_factors = self.user_item_matrix.dot(self.model.components_.T)\n",
    "            \n",
    "            # Save the model\n",
    "            model_path = os.path.join(MODELS_DIR, \"svd_model.pkl\")\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': self.model,\n",
    "                    'user_factors': self.user_factors,\n",
    "                    'item_factors': self.item_factors\n",
    "                }, f)\n",
    "            \n",
    "            print(f\"SVD model trained and saved to {model_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training SVD model: {e}\")\n",
    "            # Create dummy model components\n",
    "            if self.user_item_matrix is not None:\n",
    "                n_users, n_items = self.user_item_matrix.shape\n",
    "                self.model = object()  # Dummy model\n",
    "                self.user_factors = np.random.random((n_users, min(n_factors, 10)))\n",
    "                self.item_factors = np.random.random((n_items, min(n_factors, 10)))\n",
    "                self.model.components_ = np.random.random((min(n_factors, 10), n_items))\n",
    "            return False\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load a trained model\"\"\"\n",
    "        model_path = os.path.join(MODELS_DIR, \"svd_model.pkl\")\n",
    "        \n",
    "        try:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            \n",
    "            self.model = model_data['model']\n",
    "            self.user_factors = model_data['user_factors']\n",
    "            self.item_factors = model_data['item_factors']\n",
    "            \n",
    "            # Load mappings if not already loaded\n",
    "            if self.user_mapping is None or self.recipe_mapping is None:\n",
    "                self.load_data()\n",
    "            \n",
    "            print(\"Loaded trained SVD model\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading SVD model: {e}\")\n",
    "            print(\"Training a new model instead...\")\n",
    "            return self.train()\n",
    "    \n",
    "    def recommend(self, user_id, top_n=10):\n",
    "        \"\"\"Generate recommendations for a user\"\"\"\n",
    "        # Make sure model is loaded\n",
    "        if self.model is None:\n",
    "            success = self.load_model()\n",
    "            if not success:\n",
    "                return self._recommend_popular_items(top_n)\n",
    "        \n",
    "        # Check if user exists in our data\n",
    "        if user_id not in self.user_mapping['user_to_idx']:\n",
    "            print(f\"User {user_id} not found in training data\")\n",
    "            return self._recommend_popular_items(top_n)\n",
    "        \n",
    "        try:\n",
    "            # Get user index\n",
    "            user_idx = self.user_mapping['user_to_idx'][user_id]\n",
    "            \n",
    "            # Get user factors\n",
    "            user_factors = self.user_factors[user_idx]\n",
    "            \n",
    "            # Calculate predicted ratings for all items\n",
    "            predictions = []\n",
    "            \n",
    "            for recipe_id, recipe_idx in self.recipe_mapping['recipe_to_idx'].items():\n",
    "                # Use dot product to calculate rating\n",
    "                pred_rating = np.dot(user_factors, self.model.components_[:, recipe_idx])\n",
    "                predictions.append((recipe_id, pred_rating))\n",
    "            \n",
    "            # Sort by predicted rating\n",
    "            predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            return predictions[:top_n]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating recommendations: {e}\")\n",
    "            return self._recommend_popular_items(top_n)\n",
    "    \n",
    "    def _recommend_popular_items(self, top_n=10):\n",
    "        \"\"\"Fallback to recommend popular items\"\"\"\n",
    "        print(\"Using popular item recommendations\")\n",
    "        \n",
    "        # Try to load interactions to find popular items\n",
    "        try:\n",
    "            interactions_df = pd.read_csv(os.path.join(DATA_DIR, \"synthetic_interactions.csv\"))\n",
    "            item_counts = interactions_df['recipe_id'].value_counts()\n",
    "            return [(item, count) for item, count in item_counts.head(top_n).items()]\n",
    "        except:\n",
    "            # Return random items with popularity scores\n",
    "            return [(i, 1.0) for i in range(1, top_n + 1)]\n",
    "\n",
    "\n",
    "class ContentBasedFilteringModel:\n",
    "    \"\"\"Content-based filtering model\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.recipe_features = None\n",
    "        self.user_profiles = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load data for content-based filtering\"\"\"\n",
    "        try:\n",
    "            # Load recipe features\n",
    "            self.recipe_features = pd.read_csv(os.path.join(DATA_DIR, \"recipe_features.csv\"))\n",
    "            \n",
    "            # Load user profiles\n",
    "            self.user_profiles = pd.read_csv(os.path.join(DATA_DIR, \"user_profiles.csv\"))\n",
    "            \n",
    "            print(\"Loaded data for content-based filtering\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading content-based filtering data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the content-based model\"\"\"\n",
    "        print(\"Training content-based filtering model...\")\n",
    "        \n",
    "        # Load data if not already loaded\n",
    "        if self.recipe_features is None or self.user_profiles is None:\n",
    "            success = self.load_data()\n",
    "            if not success:\n",
    "                print(\"Failed to load data for content-based filtering\")\n",
    "                return False\n",
    "        \n",
    "        # No actual training needed for basic content-based filtering\n",
    "        # We're just using the precomputed user profiles and recipe features\n",
    "        \n",
    "        print(\"Content-based model ready\")\n",
    "        return True\n",
    "    \n",
    "    def recommend(self, user_id, top_n=10, dietary_preferences=None):\n",
    "        \"\"\"Generate recommendations based on user profile\"\"\"\n",
    "        # Load data if not already loaded\n",
    "        if self.recipe_features is None or self.user_profiles is None:\n",
    "            success = self.load_data()\n",
    "            if not success:\n",
    "                print(\"Failed to load data for content-based filtering\")\n",
    "                return []\n",
    "        \n",
    "        # Check if user exists in profiles\n",
    "        if user_id not in self.user_profiles['user_id'].values:\n",
    "            print(f\"User {user_id} not found in profiles\")\n",
    "            return self._recommend_random_items(top_n)\n",
    "        \n",
    "        try:\n",
    "            # Get user profile\n",
    "            user_profile = self.user_profiles[self.user_profiles['user_id'] == user_id]\n",
    "            \n",
    "            if len(user_profile) == 0:\n",
    "                return self._recommend_random_items(top_n)\n",
    "            \n",
    "            user_profile = user_profile.iloc[0]\n",
    "            \n",
    "            # Calculate similarity with all recipes\n",
    "            similarities = []\n",
    "            \n",
    "            for _, recipe in self.recipe_features.iterrows():\n",
    "                recipe_id = recipe['recipe_id']\n",
    "                \n",
    "                # Extract feature vectors (excluding IDs)\n",
    "                user_features = user_profile.drop('user_id').values\n",
    "                recipe_features = recipe.drop('recipe_id').values\n",
    "                \n",
    "                # Ensure vectors are the same length\n",
    "                min_length = min(len(user_features), len(recipe_features))\n",
    "                user_features = user_features[:min_length]\n",
    "                recipe_features = recipe_features[:min_length]\n",
    "                \n",
    "                # Calculate similarity\n",
    "                similarity = np.dot(user_features, recipe_features)\n",
    "                similarities.append((recipe_id, similarity))\n",
    "            \n",
    "            # Apply dietary preferences filter if specified\n",
    "            if dietary_preferences:\n",
    "                filtered_similarities = []\n",
    "                \n",
    "                try:\n",
    "                    # Load full recipe data\n",
    "                    recipes_df = pd.read_csv(os.path.join(DATA_DIR, \"enriched_recipes.csv\"))\n",
    "                    \n",
    "                    for recipe_id, similarity in similarities:\n",
    "                        recipe_data = recipes_df[recipes_df['id'] == recipe_id]\n",
    "                        \n",
    "                        if len(recipe_data) == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        # Check if recipe matches dietary preferences\n",
    "                        # This is a simplified check - in a real system you would have more detailed filters\n",
    "                        if 'low_calorie' in dietary_preferences and recipe_data['calories'].values[0] > 500:\n",
    "                            continue\n",
    "                        \n",
    "                        filtered_similarities.append((recipe_id, similarity))\n",
    "                    \n",
    "                    similarities = filtered_similarities\n",
    "                except Exception as e:\n",
    "                    print(f\"Error applying dietary filter: {e}\")\n",
    "            \n",
    "            # Sort by similarity\n",
    "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            return similarities[:top_n]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating content-based recommendations: {e}\")\n",
    "            return self._recommend_random_items(top_n)\n",
    "    \n",
    "    def _recommend_random_items(self, top_n=10):\n",
    "        \"\"\"Fallback to recommend random items\"\"\"\n",
    "        print(\"Using random recommendations\")\n",
    "        \n",
    "        if self.recipe_features is not None:\n",
    "            # Use actual recipe IDs if available\n",
    "            recipe_ids = self.recipe_features['recipe_id'].sample(min(top_n, len(self.recipe_features))).values\n",
    "            return [(recipe_id, 0.5) for recipe_id in recipe_ids]\n",
    "        else:\n",
    "            # Return some dummy recommendations\n",
    "            return [(i, 0.5) for i in range(1, top_n + 1)]\n",
    "\n",
    "\n",
    "class IngredientSubstitutionModel:\n",
    "    \"\"\"Model for suggesting ingredient substitutions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.similarity_matrix = None\n",
    "        self.ingredient_mapping = None\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load ingredient similarity data\"\"\"\n",
    "        try:\n",
    "            # Load similarity matrix\n",
    "            self.similarity_matrix = np.load(os.path.join(DATA_DIR, \"ingredient_similarity.npy\"))\n",
    "            \n",
    "            # Load ingredient mapping\n",
    "            with open(os.path.join(DATA_DIR, \"ingredient_mapping.pkl\"), 'rb') as f:\n",
    "                self.ingredient_mapping = pickle.load(f)\n",
    "            \n",
    "            print(\"Loaded ingredient similarity data\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ingredient similarity data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def find_substitutes(self, ingredient, top_n=5, dietary_restrictions=None):\n",
    "        \"\"\"Find substitutes for a given ingredient\"\"\"\n",
    "        # Load data if not already loaded\n",
    "        if self.similarity_matrix is None or self.ingredient_mapping is None:\n",
    "            success = self.load_data()\n",
    "            if not success:\n",
    "                # Return dummy substitutes\n",
    "                return self._generate_dummy_substitutes(ingredient, top_n)\n",
    "        \n",
    "        try:\n",
    "            # Check if the ingredient exists in our mapping\n",
    "            ing_to_idx = self.ingredient_mapping['ing_to_idx']\n",
    "            idx_to_ing = self.ingredient_mapping['idx_to_ing']\n",
    "            \n",
    "            # Try to find the ingredient\n",
    "            ing_idx = None\n",
    "            \n",
    "            # Exact match\n",
    "            if ingredient in ing_to_idx:\n",
    "                ing_idx = ing_to_idx[ingredient]\n",
    "            else:\n",
    "                # Try to find a partial match\n",
    "                for ing in ing_to_idx.keys():\n",
    "                    if ingredient in ing or ing in ingredient:\n",
    "                        ing_idx = ing_to_idx[ing]\n",
    "                        break\n",
    "            \n",
    "            if ing_idx is None:\n",
    "                print(f\"Ingredient '{ingredient}' not found in database\")\n",
    "                return self._generate_dummy_substitutes(ingredient, top_n)\n",
    "            \n",
    "            # Get similarity scores\n",
    "            similarities = self.similarity_matrix[ing_idx]\n",
    "            \n",
    "            # Get indices of top similar ingredients (excluding the ingredient itself)\n",
    "            similar_indices = np.argsort(similarities)[-top_n-1:-1][::-1]\n",
    "            \n",
    "            # Convert to ingredient names and scores\n",
    "            substitutes = [(idx_to_ing[idx], similarities[idx]) for idx in similar_indices]\n",
    "            \n",
    "            # Apply dietary restrictions if needed\n",
    "            if dietary_restrictions and substitutes:\n",
    "                # This is a simplified filter - in a real system you would have more detailed data\n",
    "                if 'vegan' in dietary_restrictions:\n",
    "                    non_vegan = ['butter', 'milk', 'cream', 'cheese', 'egg', 'honey']\n",
    "                    substitutes = [(s, score) for s, score in substitutes \n",
    "                                  if not any(nv in s for nv in non_vegan)]\n",
    "                \n",
    "                if 'gluten_free' in dietary_restrictions:\n",
    "                    gluten = ['wheat', 'flour', 'pasta', 'bread']\n",
    "                    substitutes = [(s, score) for s, score in substitutes \n",
    "                                  if not any(g in s for g in gluten)]\n",
    "            \n",
    "            return substitutes\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error finding substitutes: {e}\")\n",
    "            return self._generate_dummy_substitutes(ingredient, top_n)\n",
    "    \n",
    "    def _generate_dummy_substitutes(self, ingredient, top_n=5):\n",
    "        \"\"\"Generate dummy substitutes for an ingredient\"\"\"\n",
    "        print(f\"Generating dummy substitutes for '{ingredient}'\")\n",
    "        \n",
    "        prefixes = {\n",
    "            'butter': ['olive oil', 'coconut oil', 'avocado'],\n",
    "            'milk': ['almond milk', 'soy milk', 'oat milk'],\n",
    "            'sugar': ['honey', 'maple syrup', 'stevia'],\n",
    "            'flour': ['almond flour', 'coconut flour', 'rice flour'],\n",
    "            'egg': ['flax egg', 'chia egg', 'applesauce'],\n",
    "            'meat': ['tofu', 'tempeh', 'seitan']\n",
    "        }\n",
    "        \n",
    "        # Find matching prefix\n",
    "        prefix = None\n",
    "        for p in prefixes:\n",
    "            if p in ingredient:\n",
    "                prefix = p\n",
    "                break\n",
    "        \n",
    "        if prefix:\n",
    "            # Use predefined substitutes for this type\n",
    "            substitutes = [(sub, 0.9 - i*0.1) for i, sub in enumerate(prefixes[prefix][:top_n])]\n",
    "        else:\n",
    "            # Generic substitutes\n",
    "            substitutes = [(f\"substitute_{i+1}_for_{ingredient}\", 0.9 - i*0.1) for i in range(top_n)]\n",
    "        \n",
    "        return substitutes\n",
    "\n",
    "\n",
    "class HybridRecommendationSystem:\n",
    "    \"\"\"Hybrid recommendation system combining multiple approaches\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cf_model = CollaborativeFilteringModel()\n",
    "        self.cb_model = ContentBasedFilteringModel()\n",
    "        self.substitution_model = IngredientSubstitutionModel()\n",
    "    \n",
    "    def train_models(self):\n",
    "        \"\"\"Train all component models\"\"\"\n",
    "        print(\"Training hybrid recommendation system...\")\n",
    "        \n",
    "        # Train collaborative filtering model\n",
    "        cf_success = self.cf_model.train()\n",
    "        \n",
    "        # Train content-based model\n",
    "        cb_success = self.cb_model.train()\n",
    "        \n",
    "        print(\"Hybrid recommendation system ready\")\n",
    "        return cf_success and cb_success\n",
    "    \n",
    "    def recommend(self, user_id, top_n=10, alpha=0.7, dietary_preferences=None):\n",
    "        \"\"\"Generate hybrid recommendations\"\"\"\n",
    "        print(f\"Generating recommendations for user {user_id}...\")\n",
    "        \n",
    "        # Get recommendations from both models\n",
    "        cf_recs = self.cf_model.recommend(user_id, top_n=top_n*2)\n",
    "        cb_recs = self.cb_model.recommend(user_id, top_n=top_n*2, dietary_preferences=dietary_preferences)\n",
    "        \n",
    "        # If either model failed, use the other one's recommendations\n",
    "        if not cf_recs:\n",
    "            print(\"Using only content-based recommendations\")\n",
    "            return cb_recs[:top_n]\n",
    "        \n",
    "        if not cb_recs:\n",
    "            print(\"Using only collaborative filtering recommendations\")\n",
    "            return cf_recs[:top_n]\n",
    "        \n",
    "        # Normalize scores to 0-1 range\n",
    "        def normalize_scores(recommendations):\n",
    "            scores = [score for _, score in recommendations]\n",
    "            min_score = min(scores)\n",
    "            max_score = max(scores)\n",
    "            \n",
    "            if max_score == min_score:\n",
    "                return {rid: 1.0 for rid, _ in recommendations}\n",
    "            \n",
    "            return {rid: (score - min_score) / (max_score - min_score) \n",
    "                   for rid, score in recommendations}\n",
    "        \n",
    "        # Normalize scores\n",
    "        cf_scores = normalize_scores(cf_recs)\n",
    "        cb_scores = normalize_scores(cb_recs)\n",
    "        \n",
    "        # Combine recommendations\n",
    "        combined_scores = {}\n",
    "        \n",
    "        # Use all recipe IDs from both models\n",
    "        all_recipe_ids = set(cf_scores.keys()).union(set(cb_scores.keys()))\n",
    "        \n",
    "        for recipe_id in all_recipe_ids:\n",
    "            # Get scores from both models (default to 0 if not present)\n",
    "            cf_score = cf_scores.get(recipe_id, 0)\n",
    "            cb_score = cb_scores.get(recipe_id, 0)\n",
    "            \n",
    "            # Weighted combination\n",
    "            combined_scores[recipe_id] = alpha * cf_score + (1 - alpha) * cb_score\n",
    "        \n",
    "        # Sort by score\n",
    "        sorted_recs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return sorted_recs[:top_n]\n",
    "    \n",
    "    def get_recipe_details(self, recipe_id):\n",
    "        \"\"\"Get details for a recipe\"\"\"\n",
    "        try:\n",
    "            # Load recipe data\n",
    "            recipes_df = pd.read_csv(os.path.join(DATA_DIR, \"enriched_recipes.csv\"))\n",
    "            \n",
    "            # Find the recipe\n",
    "            recipe = recipes_df[recipes_df['id'] == recipe_id]\n",
    "            \n",
    "            if len(recipe) == 0:\n",
    "                # Return dummy details\n",
    "                return {\n",
    "                    'id': recipe_id,\n",
    "                    'name': f\"Recipe {recipe_id}\",\n",
    "                    'ingredients': f\"Ingredients for recipe {recipe_id}\",\n",
    "                    'instructions': f\"Instructions for recipe {recipe_id}\",\n",
    "                    'nutrition': {\n",
    "                        'calories': 500,\n",
    "                        'protein': 20,\n",
    "                        'fat': 15,\n",
    "                        'carbs': 60,\n",
    "                        'fiber': 5,\n",
    "                        'sugar': 10,\n",
    "                        'sodium': 400\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # Extract details\n",
    "            recipe = recipe.iloc[0]\n",
    "            \n",
    "            details = {\n",
    "                'id': recipe_id,\n",
    "                'name': recipe.get('recipe_name', f\"Recipe {recipe_id}\"),\n",
    "                'ingredients': recipe.get('ingredients', \"\"),\n",
    "                'instructions': recipe.get('instructions', \"\"),\n",
    "                'nutrition': {\n",
    "                    'calories': recipe.get('calories', 0),\n",
    "                    'protein': recipe.get('protein', 0),\n",
    "                    'fat': recipe.get('fat', 0),\n",
    "                    'carbs': recipe.get('carbs', 0),\n",
    "                    'fiber': recipe.get('fiber', 0),\n",
    "                    'sugar': recipe.get('sugar', 0),\n",
    "                    'sodium': recipe.get('sodium', 0)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return details\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting recipe details: {e}\")\n",
    "            \n",
    "            # Return dummy details\n",
    "            return {\n",
    "                'id': recipe_id,\n",
    "                'name': f\"Recipe {recipe_id}\",\n",
    "                'ingredients': f\"Ingredients for recipe {recipe_id}\",\n",
    "                'instructions': f\"Instructions for recipe {recipe_id}\",\n",
    "                'nutrition': {\n",
    "                    'calories': 500,\n",
    "                    'protein': 20,\n",
    "                    'fat': 15,\n",
    "                    'carbs': 60,\n",
    "                    'fiber': 5,\n",
    "                    'sugar': 10,\n",
    "                    'sodium': 400\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def find_substitutes(self, ingredient, top_n=5, dietary_restrictions=None):\n",
    "        \"\"\"Find substitutes for an ingredient\"\"\"\n",
    "        return self.substitution_model.find_substitutes(\n",
    "            ingredient, top_n=top_n, dietary_restrictions=dietary_restrictions\n",
    "        )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main Application\n",
    "# =============================================================================\n",
    "\n",
    "class RecipeRecommendationSystem:\n",
    "    \"\"\"Main application class\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_loader = DataLoader()\n",
    "        self.preprocessor = DataPreprocessor()\n",
    "        self.recommender = HybridRecommendationSystem()\n",
    "    \n",
    "    def setup_system(self, force_processing=False):\n",
    "        \"\"\"Set up the recommendation system\"\"\"\n",
    "        print(\"Setting up the recommendation system...\")\n",
    "        \n",
    "        # Check if processed data exists\n",
    "        enriched_path = os.path.join(DATA_DIR, \"enriched_recipes.csv\")\n",
    "        interactions_path = os.path.join(DATA_DIR, \"synthetic_interactions.csv\")\n",
    "        \n",
    "        processed_data_exists = os.path.exists(enriched_path) and os.path.exists(interactions_path)\n",
    "        \n",
    "        if not processed_data_exists or force_processing:\n",
    "            print(\"Processing data from scratch...\")\n",
    "            \n",
    "            # Load and process data\n",
    "            data = self.data_loader.load_all_data()\n",
    "            \n",
    "            # Preprocess data\n",
    "            preprocessed = self.preprocessor.preprocess_data()\n",
    "        else:\n",
    "            print(\"Loading existing processed data...\")\n",
    "            self.preprocessor.load_data()\n",
    "        \n",
    "        # Train recommendation models\n",
    "        self.recommender.train_models()\n",
    "        \n",
    "        print(\"System setup complete!\")\n",
    "    \n",
    "    def get_recommendations(self, user_id, top_n=10, dietary_preferences=None):\n",
    "        \"\"\"Get personalized recommendations for a user\"\"\"\n",
    "        # Get recommendations\n",
    "        recommendations = self.recommender.recommend(\n",
    "            user_id, top_n=top_n, dietary_preferences=dietary_preferences\n",
    "        )\n",
    "        \n",
    "        # Get detailed information for each recommendation\n",
    "        detailed_recommendations = []\n",
    "        \n",
    "        for recipe_id, score in recommendations:\n",
    "            details = self.recommender.get_recipe_details(recipe_id)\n",
    "            details['score'] = score\n",
    "            detailed_recommendations.append(details)\n",
    "        \n",
    "        return detailed_recommendations\n",
    "    \n",
    "    def search_recipes(self, query, top_n=10):\n",
    "        \"\"\"Search for recipes by keyword\"\"\"\n",
    "        try:\n",
    "            # Load recipe data\n",
    "            recipes_df = pd.read_csv(os.path.join(DATA_DIR, \"enriched_recipes.csv\"))\n",
    "            \n",
    "            # Convert query to lowercase\n",
    "            query = query.lower()\n",
    "            \n",
    "            # Search in recipe names and ingredients\n",
    "            matches = []\n",
    "            \n",
    "            for _, recipe in recipes_df.iterrows():\n",
    "                name = str(recipe.get('recipe_name', '')).lower()\n",
    "                ingredients = str(recipe.get('ingredients', '')).lower()\n",
    "                \n",
    "                # Calculate relevance score\n",
    "                name_match = query in name\n",
    "                ingredients_match = query in ingredients\n",
    "                \n",
    "                score = 0\n",
    "                if name_match:\n",
    "                    score += 2\n",
    "                if ingredients_match:\n",
    "                    score += 1\n",
    "                \n",
    "                # Include partial matches\n",
    "                if score == 0:\n",
    "                    for term in query.split():\n",
    "                        if len(term) > 3:  # Ignore short terms\n",
    "                            if term in name:\n",
    "                                score += 0.5\n",
    "                            if term in ingredients:\n",
    "                                score += 0.25\n",
    "                \n",
    "                if score > 0:\n",
    "                    matches.append((recipe['id'], score))\n",
    "            \n",
    "            # Sort by relevance\n",
    "            matches.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Get detailed information\n",
    "            search_results = []\n",
    "            \n",
    "            for recipe_id, score in matches[:top_n]:\n",
    "                details = self.recommender.get_recipe_details(recipe_id)\n",
    "                details['relevance'] = score\n",
    "                search_results.append(details)\n",
    "            \n",
    "            return search_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching recipes: {e}\")\n",
    "            return []\n",
    "        \n",
    "    def evaluate_recommendations(self, test_df):\n",
    "     \"\"\"Evaluate the recommendation system using MSE and RMSE on test data\"\"\"\n",
    "     print(\"Evaluating recommendation system...\")\n",
    "    \n",
    "     try:\n",
    "        # Get actual ratings from test data\n",
    "        test_ratings = {}\n",
    "        for _, row in test_df.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            recipe_id = row['recipe_id']\n",
    "            rating = row['rating']\n",
    "            test_ratings[(user_id, recipe_id)] = rating\n",
    "        \n",
    "        # Generate predictions for test data\n",
    "        predictions = []\n",
    "        actual_ratings = []\n",
    "        \n",
    "        for (user_id, recipe_id), actual_rating in test_ratings.items():\n",
    "            # Get user's predicted rating for this recipe\n",
    "            # We'll use the hybrid model with alpha=0.7 (or your preferred setting)\n",
    "            user_recs = self.recommender.recommend(user_id, top_n=100, alpha=0.7)\n",
    "            \n",
    "            # Find the recipe in recommendations if it exists\n",
    "            predicted_rating = None\n",
    "            for rid, score in user_recs:\n",
    "                if rid == recipe_id:\n",
    "                    # Scale the score from 0-1 to 1-5 rating scale\n",
    "                    predicted_rating = 1 + score * 4\n",
    "                    break\n",
    "            \n",
    "            # If recipe was not in recommendations, use average rating as fallback\n",
    "            if predicted_rating is None:\n",
    "                # Either use global average or user's average\n",
    "                user_ratings = test_df[test_df['user_id'] == user_id]['rating']\n",
    "                if len(user_ratings) > 0:\n",
    "                    predicted_rating = user_ratings.mean()\n",
    "                else:\n",
    "                    predicted_rating = test_df['rating'].mean()\n",
    "            \n",
    "            predictions.append(predicted_rating)\n",
    "            actual_ratings.append(actual_rating)\n",
    "        \n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(actual_ratings, predictions)\n",
    "        \n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # Calculate MAE (Mean Absolute Error)\n",
    "        mae = np.mean(np.abs(np.array(actual_ratings) - np.array(predictions)))\n",
    "        \n",
    "        # Calculate RÂ² score\n",
    "        r2 = 1 - (np.sum((np.array(actual_ratings) - np.array(predictions))**2) / \n",
    "                  np.sum((np.array(actual_ratings) - np.mean(actual_ratings))**2))\n",
    "        \n",
    "        print(f\"Evaluation results:\")\n",
    "        print(f\"  MSE:  {mse:.4f}\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  MAE:  {mae:.4f}\")\n",
    "        print(f\"  RÂ²:   {r2:.4f}\")\n",
    "        \n",
    "        # Save results to file\n",
    "        evaluation_path = os.path.join(RESULTS_DIR, \"evaluation_results.txt\")\n",
    "        with open(evaluation_path, 'w') as f:\n",
    "            f.write(f\"Evaluation Results\\n\")\n",
    "            f.write(f\"=================\\n\")\n",
    "            f.write(f\"MSE:  {mse:.4f}\\n\")\n",
    "            f.write(f\"RMSE: {rmse:.4f}\\n\")\n",
    "            f.write(f\"MAE:  {mae:.4f}\\n\")\n",
    "            f.write(f\"RÂ²:   {r2:.4f}\\n\")\n",
    "        \n",
    "        # Create visualization of actual vs predicted ratings\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(actual_ratings, predictions, alpha=0.5)\n",
    "        plt.plot([1, 5], [1, 5], 'r--')  # Diagonal line for perfect predictions\n",
    "        plt.xlabel('Actual Ratings')\n",
    "        plt.ylabel('Predicted Ratings')\n",
    "        plt.title('Actual vs Predicted Ratings')\n",
    "        plt.xlim(0.5, 5.5)\n",
    "        plt.ylim(0.5, 5.5)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save the plot\n",
    "        plot_path = os.path.join(RESULTS_DIR, \"rating_predictions.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Evaluation results saved to {evaluation_path}\")\n",
    "        print(f\"Prediction plot saved to {plot_path}\")\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'num_samples': len(actual_ratings)\n",
    "        }\n",
    "        \n",
    "     except Exception as e:\n",
    "        print(f\"Error evaluating recommendations: {e}\")\n",
    "        return {\n",
    "            'mse': None,\n",
    "            'rmse': None,\n",
    "            'mae': None,\n",
    "            'r2': None,\n",
    "            'num_samples': 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "    \n",
    "    def find_substitutes(self, ingredient, top_n=5, dietary_restrictions=None):\n",
    "        \"\"\"Find substitutes for an ingredient\"\"\"\n",
    "        return self.recommender.find_substitutes(\n",
    "            ingredient, top_n=top_n, dietary_restrictions=dietary_restrictions\n",
    "        )\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Visualization Functions\n",
    "# =============================================================================\n",
    "\n",
    "def plot_rating_distribution():\n",
    "    \"\"\"Plot distribution of ratings\"\"\"\n",
    "    try:\n",
    "        # Load interaction data\n",
    "        interactions_path = os.path.join(DATA_DIR, \"synthetic_interactions.csv\")\n",
    "        interactions_df = pd.read_csv(interactions_path)\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(x='rating', data=interactions_df)\n",
    "        plt.title('Distribution of Recipe Ratings')\n",
    "        plt.xlabel('Rating')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Save plot\n",
    "        plot_path = os.path.join(RESULTS_DIR, \"rating_distribution.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Rating distribution plot saved to {plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating rating distribution plot: {e}\")\n",
    "\n",
    "def plot_nutrition_distribution():\n",
    "    \"\"\"Plot distribution of nutritional values\"\"\"\n",
    "    try:\n",
    "        # Load recipe data\n",
    "        recipes_path = os.path.join(DATA_DIR, \"enriched_recipes.csv\")\n",
    "        recipes_df = pd.read_csv(recipes_path)\n",
    "        \n",
    "        # Create plots for each nutritional attribute\n",
    "        nutrients = ['calories', 'protein', 'fat', 'carbs', 'fiber', 'sugar', 'sodium']\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        for i, nutrient in enumerate(nutrients, 1):\n",
    "            if nutrient in recipes_df.columns:\n",
    "                plt.subplot(3, 3, i)\n",
    "                sns.histplot(recipes_df[nutrient], kde=True)\n",
    "                plt.title(f'Distribution of {nutrient.capitalize()}')\n",
    "                plt.xlabel(nutrient.capitalize())\n",
    "                plt.ylabel('Count')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plot_path = os.path.join(RESULTS_DIR, \"nutrition_distribution.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Nutrition distribution plot saved to {plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating nutrition distribution plot: {e}\")\n",
    "\n",
    "def plot_user_recipe_heatmap(num_users=20, num_recipes=30):\n",
    "    \"\"\"Create a heatmap of user-recipe interactions\"\"\"\n",
    "    try:\n",
    "        # Load interaction data\n",
    "        interactions_path = os.path.join(DATA_DIR, \"synthetic_interactions.csv\")\n",
    "        interactions_df = pd.read_csv(interactions_path)\n",
    "        \n",
    "        # Get top users and recipes by interaction count\n",
    "        top_users = interactions_df['user_id'].value_counts().head(num_users).index\n",
    "        top_recipes = interactions_df['recipe_id'].value_counts().head(num_recipes).index\n",
    "        \n",
    "        # Filter interactions\n",
    "        filtered = interactions_df[\n",
    "            interactions_df['user_id'].isin(top_users) & \n",
    "            interactions_df['recipe_id'].isin(top_recipes)\n",
    "        ]\n",
    "        \n",
    "        # Create pivot table\n",
    "        pivot = filtered.pivot_table(\n",
    "            index='user_id',\n",
    "            columns='recipe_id',\n",
    "            values='rating',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        sns.heatmap(pivot, cmap='YlGnBu')\n",
    "        plt.title('User-Recipe Rating Heatmap')\n",
    "        plt.xlabel('Recipe ID')\n",
    "        plt.ylabel('User ID')\n",
    "        \n",
    "        # Save plot\n",
    "        plot_path = os.path.join(RESULTS_DIR, \"user_recipe_heatmap.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"User-recipe heatmap saved to {plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating user-recipe heatmap: {e}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main Function\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for the Recipe Recommendation System\"\"\"\n",
    "    # Initialize the system\n",
    "    system = RecipeRecommendationSystem()\n",
    "    \n",
    "    # Setup the system\n",
    "    system.setup_system(force_processing=True)\n",
    "    \n",
    "    # Create visualizations\n",
    "    plot_rating_distribution()\n",
    "    plot_nutrition_distribution()\n",
    "    plot_user_recipe_heatmap()\n",
    "    \n",
    "    print(\"\\n=== System Ready ===\\n\")\n",
    "\n",
    "    # Load test data for evaluation\n",
    "    test_path = os.path.join(DATA_DIR, \"test_interactions.csv\")\n",
    "    if os.path.exists(test_path):\n",
    "      test_df = pd.read_csv(test_path)\n",
    "      print(\"\\n=== Evaluating System Performance ===\\n\")\n",
    "    \n",
    "    # If test set is very large, use a sample for faster evaluation\n",
    "      if len(test_df) > 30:  # You can adjust this threshold as needed\n",
    "        test_sample = test_df.sample(30, random_state=42)\n",
    "        evaluation_results = system.evaluate_recommendations(test_sample)\n",
    "        print(f\"Evaluation performed on {len(test_sample)} random samples from test set\")\n",
    "      else:\n",
    "        evaluation_results = system.evaluate_recommendations(test_df)\n",
    "    \n",
    "    # Use the evaluation results\n",
    "      if evaluation_results and 'rmse' in evaluation_results and evaluation_results['rmse'] is not None:\n",
    "        print(f\"\\nSummary of evaluation metrics:\")\n",
    "        print(f\"  Number of samples: {evaluation_results['num_samples']}\")\n",
    "        print(f\"  RMSE: {evaluation_results['rmse']:.4f}\")\n",
    "        \n",
    "        # You could also perform additional actions based on the results\n",
    "        if evaluation_results['rmse'] < 1.0:\n",
    "            print(\"Model performance is excellent!\")\n",
    "        elif evaluation_results['rmse'] < 1.5:\n",
    "            print(\"Model performance is good\")\n",
    "        else:\n",
    "            print(\"Model could benefit from further improvements\")\n",
    "    \n",
    "    # Example 1: Get personalized recommendations\n",
    "    user_id = 1  # Use an appropriate user ID from your data\n",
    "    print(f\"Getting personalized recommendations for user {user_id}...\")\n",
    "    recommendations = system.get_recommendations(\n",
    "        user_id, \n",
    "        top_n=5,\n",
    "        dietary_preferences=['low_calorie']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRecommended Recipes:\")\n",
    "    for i, recipe in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {recipe['name']} (Score: {recipe['score']:.3f})\")\n",
    "        print(f\"   Calories: {recipe['nutrition']['calories']:.1f}, \"\n",
    "              f\"Protein: {recipe['nutrition']['protein']:.1f}g, \"\n",
    "              f\"Carbs: {recipe['nutrition']['carbs']:.1f}g\")\n",
    "        print(f\"   Ingredients: {recipe['ingredients'][:100]}...\")\n",
    "    \n",
    "    # Example 2: Search for recipes\n",
    "    search_query = \"chicken\"\n",
    "    print(f\"\\nSearching for '{search_query}' recipes...\")\n",
    "    search_results = system.search_recipes(search_query, top_n=3)\n",
    "    \n",
    "    print(\"\\nSearch Results:\")\n",
    "    for i, recipe in enumerate(search_results, 1):\n",
    "        print(f\"{i}. {recipe['name']} (Relevance: {recipe.get('relevance', 0):.1f})\")\n",
    "        print(f\"   Ingredients: {recipe['ingredients'][:100]}...\")\n",
    "    \n",
    "    # Example 3: Get ingredient substitutions\n",
    "    ingredient = \"butter\"\n",
    "    print(f\"\\nFinding substitutes for '{ingredient}'...\")\n",
    "    substitutes = system.find_substitutes(\n",
    "        ingredient, \n",
    "        top_n=3,\n",
    "        dietary_restrictions=['vegan']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPossible Substitutes:\")\n",
    "    for substitute, similarity in substitutes:\n",
    "        print(f\"- {substitute} (Similarity: {similarity:.2f})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
